REWARD MODEL:
  RewardModel(
  (supervised_baseline): PegasusForConditionalGeneration(
    (model): PegasusModel(
      (shared): Embedding(96103, 1024, padding_idx=0)
      (encoder): PegasusEncoder(
        (embed_tokens): Embedding(96103, 1024, padding_idx=0)
        (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)
        (layers): ModuleList(
          (0): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (1): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (2): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (3): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (4): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (5): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (6): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (7): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (8): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (9): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (10): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (11): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (12): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (13): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (14): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (15): PegasusEncoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
      (decoder): PegasusDecoder(
        (embed_tokens): Embedding(96103, 1024, padding_idx=0)
        (embed_positions): PegasusSinusoidalPositionalEmbedding(1024, 1024)
        (layers): ModuleList(
          (0): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (1): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (2): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (3): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (4): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (5): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (6): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (7): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (8): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (9): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (10): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (11): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (12): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (13): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (14): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
          (15): PegasusDecoderLayer(
            (self_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (encoder_attn): PegasusAttention(
              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
            )
            (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (lm_head): Linear(in_features=1024, out_features=96103, bias=False)
  )
  (head): Linear(in_features=1024, out_features=1, bias=True)
)
	query_tensors: 
	 tensor([[  903,  8680,   189, 10702,  3814,   108,   130,   125,   346,   124,
           161,   685,   107,  1633,   147, 30058,   151,   125,   346,   114,
           281,   399,   980,   375,   297,   121,  1139,   134,   114,  1705,
           107,   139,   295,   241,   125,   201,  2841,   149,   660,   113,
           425,   143,  8034,   108, 14041,   108, 12287,   108,   733,   158,
           155,   145,   163,  1364,  2003,  2147,  8031,   111,   114,   827,
           809,   113,   514,  2003,  2147, 10803,   107,   184,   133,   114,
           357,   121,  1139,  2644, 43227,   120,  6260,   111,  7483,   124,
           109,  8031,   108,   155,   178,   209,   659,   333,   109,   242,
           107,   222,   109,  1709,   108,   109, 27130,   116,   143,   187,
           107,   326,   107,   213,   158,   369,   112,  1094,   124,   109,
          8031,   118,   527,   107, 22709,   151,   125,   140,   375,   124,
           109,  1325,  2337,   333,   114,  3330,  2117,   565,   136,   555,
          1342,   107,   202,  1590,  8270,   109,  3029,  2605,   114,  1209,
          1344,  2003,  2147,  2644,   107,  1593,   131,   116,   188,   443,
           215,  2644,  1590, 33289,   107,  2587,   151,  4451,   186,   147,
          5013,   119,   172,   181,   724,   124,   186,   152, 33289,   151,
         10247,   108,   125,   217,   114,  2644,   120, 10583, 52435,   200,
           107,   125,   116,   136,   461,   505,   152,  2587,   151,  1894,
           120,   117,   109,  1209,   628,   108,   167,   126,   209,  2841,
         28949,   107,   139,   423,  8031,  1278, 52435,   107, 33289,   151,
          4384,   108,   137,   119,   403,   213,   241,   274,   127,   152,
          2587,   151,  7435,   147,   125,   237,   687,   204,   112,   109,
          1381, 55195,   111,  2375,   215,   181,  8031,   107,   452,  3436,
           120,   265,   869,   364,   120,   243,  4098,  7451,   506,   167,
           125,   192,   188,   133,   112,  1094,   109,   442,   107, 12974,
           265,   140,   115,   114, 12373,   107,   125,  3498,   181,  8031,
           108,   115,   970,   156,   120,   140,   708,   785,   122,   150,
           527,   107,   168,   140,  1622,   115, 10801, 44325,   316,  3918,
           111,   196,  4098,  7451,  1158,   115,   461,   108, 10801,  3439,
           107, 10013,   115,   109,   555,   133,   288,  1314,   136,  2644,
           108,   302,   118,  3185,   111,  2220,   107,   434,   125,  5459,
           126,   165,   108,   265, 81670,   107, 33289,   151,  4384,  1467,
           108,   220,   108,   125,   137,   131,   144,   179,   120,   156,
           107,   168,   131,   116,   314,  7144,  2916,   107,   285,   143,
           544,  2593,   265,   140,  1783,   109,  2644,   118,   158,   138,
          3832,   213,   175,   125,   179,   120,   147,  1032,   117,   864,
           114,   234,   166,   112,  2721,   120,   125,   346,   114, 39991,
           758,  5333,   170,   117,   114,  1124, 14853,   113,  8568,  1420,
           111,   253,   107,   125,  1501,  1552,  1909,  1505,   173,   265,
           243,   120,   107,   110, 50036,   213,   152, 16737, 84852, 15320,
           108,   220,   107,   139, 39320,   151,   452,  1477,  3093,   165,
           114,  2644,   111,   898,   213,   112,  1094,   114,   442,   124,
           126,   108,   111,   112, 12373,   107,   125,  1159,   136,  1590,
           690,   112,  4512,   108,   254,   175,   126,   195,   188,   115,
           114, 30696,   230,   107,   412,   125,   635,   161,   110,   116,
          2625, 57190,   166,   107,   125,  1049,   215,   180,  2142,   108,
          8980,   500,  6039,   108,  2736,   108,   733,   107,   125,   237,
           108,   130,  3642,   130,   433,   108,  2021,   237,   442,   115,
           161,  3741,   561,   724,   107,  2267,   120,   107,   434,   532,
          2375,   215,   109,  2644,   108,   265,   140,  4368, 88203,   122,
           109,   711,   108,   155,   125,  8018,   265,  1373,  1021,   112,
           416,   742,   262,   125,  3436,   120,   125,  1417,   131,   144,
           114,  1209, 43227,   143,   187,   589,  9402,  2911,   432,   111,
           125,   172,   112,   416,   125,   131,   208,   848,   234,   110,
           151,   470,   250,   452,  1389,   111,   140,   124,   215,   230,
           107,   125,  1363,   109,  1004,   113,   109,  1709, 14395,   115,
           109, 39956,   113,   161, 30696, 17341,   107,     1]],
       device='cuda:0')
	response_tensors: 
	 tensor([[   0,  202, 1590, 1049,  213,  112, 1094,  215,  442,  124,  142, 2003,
         2147, 2644,  107,  125, 3498,  114, 2644,  120,  140,  785,  122,  527,
          108,  155,  265,  243,  126,  140,  314, 8568,  107,  125, 2021,  215,
          442,  115,  161, 3741,  561,  724,  107,    1]], device='cuda:0')
	query_tensors: 
	 tensor([[ 6187,  5627,   113,   109,  1289,   108,   145,   547,  2321,   173,
           125,   140,   114, 51927,   111,   265,   140,   114,  7273,   107,
           182,   140,   161,   211,  1289,   111,   215,   453,   108,   122,
           215,   211,  3793,   215,   114,   367,   113, 53104,   111,   270,
         23137, 82556,   107,   409,   209,   698, 12768,   195,   204,   146,
          1749, 77681,   432,   505,   111,   213,   146,  2536,  1090,   112,
           215,   277,   145,   195,  2971,   107,   139,   951,   140,   112,
          1178,   539,   290,   242,   424,   108,   162,   265,   140,   149,
           118,   111,   125,  3139,   112,   172,  2409,  2061, 19619,   107,
         59133,   271,   148,   196,  2111,  1660,   111, 71367,   985,  5668,
           108,   155,  1380,   848,   234,   107,   452,   148,   174, 23094,
           112,   696,   165,   113,   215,  3113,   131,   116,   480,   118,
           204,   114,   232,   239,   108,   111,   265,  1477,   374,   114,
           295,   122,   114,  1151,   135,   201,  1126,  9613,   627,  1100,
           111,   169, 10099,  1126,  6304,  1091,  5431,   139,   511,   140,
           118,   213,   112,   696,   115,   122,   215,   559,   125,  1554,
           161,   453,  6675,   113,  1416,   143, 27752,   396,   117,   352,
           396,  4038,   155,   120,   148,   174,   414,   124,  1137,   107,
          3694,  1652,   115,   122,   215,  1151,   265,  3530,   213,   122,
           342,   130,   215,   275,   112,  2593,   118,   557,   742,   122,
           107,   409, 48393,   271, 27757,   112,   114, 15543,   108,   111,
           265,   744,  4820,   164,   122,   213,   262,   265,  1373,   120,
           126,  1417,   131,   144,  2335,   112,   213,   118,   215,   112,
           133,   647,   265,   869,   112,   129,   122,   154,   197,   213,
           107,   184,  2371,   164,  3807,   424,   108,   155,   145,   137,
           302,  1989,   120,   150,  1289,   148,  1166,   181,   113,   109,
         13436,  3357,   546,   120,   126,   263,   112,   133,   107,  3054,
           113,  4282,   126,   299,   577,   108,   145,   245,   112,   508,
           111,   193,   270,   424,   130,   546,   130,   126,   263,   112,
           129,   107,   125,   235,   126,   875,   131,   144,   770,   120,
           145,   137,   935,   112,   199,   126,   140,   115,   390,  1871,
           141,   108,   155,   145,   288,   217,   225,   124,   199,   112,
           193,   150,  1289,  1281,   109,   804,   113,   166,   107,     1]],
       device='cuda:0')
	response_tensors: 
	 tensor([[    0,   600, 10099,   111,   125,   133,  1166,   181,   113,   109,
           546,   120,   145,   263,   112,   133,   115,   150,  1289,   108,
           155,   145,   245,   112,   508,   111,   193,   126,   130,   546,
           130,   126,   263,   112,   129,   107,     1]], device='cuda:0')
	query_tensors: 
	 tensor([[  377,   590,   165,   113,   114, 19872,  5752,  1019,   124,   191,
          2467, 51715,  1289, 24391,   122,  3863,   108, 34536,   609,   121,
         85673, 48393,   108, 19955,   108,  4439,   115, 25408,  4897,   122,
         72065,  2288,  4026,   107,   182,  1289,  3415, 26352,   213,   164,
           107,   125,   133,  1650,   618,   122,   200,   279,   213,   111,
          1150,   108,   125,  2767,   161, 14733,   113,   536,   111,  1150,
           401,  2409,   458,   114,   221,   234,   271,   233,   125,   498,
           115,  1169,   130,   142, 35990,   108,   201,   115,  3324,   108,
           498,   221,  8636,   121,  4401,   112,  2696,   206,  4748, 22686,
          2409,   329,   270,  2117,   108, 63033,   733,   107,   108,   573,
          1135,   113,   813,  1092,   107,   125,  1373,   172,   114, 30981,
           115,   114,  3382,  2094,   108,   170,   419,   109,   271, 69871,
          4105,   113,   215,   111,   117,  2402, 85413,   107,   125,   131,
           208,   188,  1215,   112,  1644,  5830,   115,   109,   555,   296,
           590,   107,  2882,  1327,   125,   670,   114,  2593,   134,   114,
          1151,   131,   116,  2453,   829,   107,   285,   131,   116,  2402,
           291,   135,  2266,   115,  1169,   125,   832,   670,   107,  5938,
           112,  2776,   108,  3765,   108,  5983,   161, 16126,  2325,  3903,
           107,   184,  1194,   126,   299,   107,   285,  6937,   118,   161,
           344,   233,   125,  8891,   111, 22286,   165,   111,   416,   364,
           172,   125,   272,   131,   144,   245,   112,   361,   161,   344,
           115,   683,   113,   161,  3719,   594,   170,   137,   129, 24285,
           116,   107,   413,   193,   164,   118,   126,   108,   125,   854,
           118,   169,   289,   442,   112,   535,   124,  1208,   107,   285,
           649,   178,   131,   116,  2857,   178,   131,   116,  1676,   213,
           108,   114,  4371,   113,   176,   948,  1549,   108,   120,   178,
           131,   267,   179,   115,  1266,   111,  1728,   112,   236,   213,
           435,   107,   125,   535,   342,   124, 15721,   173,   125,   179,
           238,   108,  1053,   342,   114,   221,  5048,  1285,   120,   126,
           140,   255,   112,   670,   342,   107,  2508,   390,   678,   178,
         16179,   120,   126,   140,   255,   112,   670,   213,   314,   111,
           178,  3840,   112,   236,   213,   783,   107,   125,   272,   131,
           144,   235,   180,   112,  2847,   108,   175,   178,   140,   589,
          1161,   132,   188,   270, 15354,   108,   175,   125,   246,   361,
           342,   161,   344,   239,   152,   139,   213,   135,   228,   231,
           754,   192,   131,   261,   606,  1270,   180,   112,   171,   111,
           192,   131,   261,   174,   167, 10761,   160,   136,   155,   125,
           393,   172,   125,   131,   261,  1166,   120,  2285, 20694,   120,
          3387,   119,   180,   112,   171,   115,   219,  2673,   113,  2806,
           107,   125,   192,   288,   172,   112,   179,   112,   235,   342,
           155,   133,   220,   641,   180,   112,   171,   352,   107,     1]],
       device='cuda:0')
	response_tensors: 
	 tensor([[    0,   125,  1676,   114,  2593,   134,   114,  1151,   131,   116,
          2453,   829,   108,   178,   649,   178,   131,   116,  2857,   178,
           131,   116,  1676,   213,   111,  1728,   112,   236,   213,   435,
           107,   125,   272,   131,   144,   235,   180,   112,   171,   108,
           175,   178,   140,  1161,   132,   188,   270, 15354,   108,   175,
           125,   246,   361,   342,   161,   344,   239,   152,     1]],
       device='cuda:0')
	query_tensors: 
	 tensor([[ 2859,   584,   613,   161,  9066,   111,   125,  4820,   164,  3134,
           107,   184,  2255,  4820,   164,   118,   114,   613,   908,   113,
           166,   289,  1817,   155,   419,   247,   424,   111,   341,   195,
           557,  1226,   107,   409,  1289,   140,   221,  1651,   560,   145,
         15684,   424,   290,   565,   108,  1363,   744,   149,   294,   166,
           424,   108,   125,  2703,   115,   169,   480,   122,   169,   328,
           118,   114,   932,   113,   590,   204,   109,   922,   191, 62437,
           112,   153,   480,   118,   109,  3578,   108,  5306, 37410,  1151,
          1211,   107,   125,   272,   131,   144,   311,   145,   127,   313,
           112,   179,   247,   424,   136,   166,   111,   125,   131,   208,
           288, 33533,   262,   125,   140,   288,   124,  1103,   118,   214,
           112,   753,   424,   430,   178,  7081,   135,   109,  1416,   145,
           302,   275,   112,   115,  1185,   107,  1032,   178,  8677,   150,
           664,   300,  1289,   429,   118,   114,   324,   590,   113,   458,
           109,   198, 51955,  1416,   306,   194,   269,   178,  2163,   107,
           125,  1786,   131,   144,  4337,   112,   342,   381,   289,   565,
           173,   178,   518,   107, 12675,   145,   138,   217,   112,  9333,
          1549,   247,   381,   178,   148,   114,   324, 32575,   113,  2643,
           111,  5070, 15917,   143,   187,   848,   249,  2703,   115,   169,
         22066,   418,   122,   342,   880,   197,   161,   282,   250,   285,
          2163,  3469,   112,   275,   165,   113,   531,   118,   114,   396,
           167,   125,  2767,   145,   138,  1799,   269,   237,   107,   600,
           906,   117,   151,   199,   112,   275,   198,  1804,   121, 20077,
           194,   173,   220,   121, 20077,   117,   146,   433,   152,   125,
           235,   119,  2266,   127,   313,   112,   416,   120,   126,   131,
           116,   329,   433,   155,   126,   131,   116,   146,   264,   107,
           125,   138,  1985,   236,   342,   290,   612,   242,   262,   145,
           127,   115,   109,   310,  1134,   120,   125,   138,  3626,   130,
           114,   198,  1572,   121,   316,   425, 27737,   144,   194,   241,
           186,   117,   136,   480,   241,   145,   248,   149,   113,   150,
          3215,   122,   109,   310,   456,   113,   200,   108,   149,   525,
           702,   127,   224,   136,  1666,   108,  1829,   108,   733,   107,
           125,   138,   236,   342,   134,  3215,   143,  2018,  1691,   418,
           158,   111,  1596, 35381,  9579,   125,   138,   236,   342,   134,
          1829, 47786,   164,   122,   109,  2220,   178,   131,   116, 19610,
           112,   179,   122,   239,   120,   178,   131,   116,   419,   169,
          3002,   107,   182,  1666,   117,   221,   356,   112,   213,   130,
           126,   117,   109,  1104,   113,   161,   664,  1416,   525,   271,
           111,   163,   109,  1116,   113,   161,   425,   111,   125,   346,
           114,   848,  1651,   855,   108,   167,   186,   131,   116,   220,
           440,   888,   113,  2096,   126,   107,   139,   352,   668,   590,
           140,   864,   109,   289,   166,   145,   195,   313,   112,   133,
           424,  3689,   381,   145,   195,  1062,   124,  4282,   164,   173,
           178,  7081,   143,   187,   131,   208,   114,  6222,   158,   111,
           125,   140,   288,   383,   782,   112, 79807,   136,   548,   111,
           356,   908,   113,   169,   271,   107,  1032,   149,   125,   236,
           117, 16520,   118,   213,   111,   125,   235,   120,   357,  7879,
           576,   131,   144,   331,   430,   178,   117,  1871,   107,   125,
           131,   208,   848,   249,   134,  2054,  1472,   167,   189,  1302,
           117,  5190,   107,   125,   298,   136,  2593,   122,   579,   125,
           133,   155,   125,  2665,   178,   188,   595,   131,   144,  4288,
           107,   139,   209,   176,   888,   125,  2665,   125,   133,   117,
           112,   508,   112,   753,   594,   111, 28197,   161,   773,   224,
          3411,   112,   342,   270,   122,   176,  2220,   107,     1]],
       device='cuda:0')
	response_tensors: 
	 tensor([[    0, 63137,   111,   125,  4820,   164,   118,   109,   453,   166,
           115,   478,   197,   114,   232,   262,   178,   131,   116,   313,
           112,  1416,   118,   114,   324,   590,   111,   125,   272,   131,
           144,   235,   199,   112,   275,   198,  1804,   121, 20077,   194,
           173,   220,   121, 20077,   117,   146,   433,   107,     1]],
       device='cuda:0')
	query_tensors: 
	 tensor([[  139,  7171,   151,   125,   346,  2168,   111,   115,   161,   289,
           232,   113,  1416,   107,   125,   133,   394,   174,   115,   114,
          1289,   269,   262,   125,   133,   181,   848,   461,  2284,   618,
           107,   139,  5894,   151, 10652,   779,   130,   213,   107,   125,
          1676,   215,   160,   114,   625,   754,   107, 29436,   112,  1588,
           107,  7404,  1822,   111, 19560,   108,   744,   172,   114,  2972,
           107,   182,  2326,   112,  4359,   175,   265,   117,   188,   270,
          1615,   118,   163,   270, 58525,   122,   176,  2266,   107,  1308,
           108,   265,   117,   135,  4611,   111,   215,  1188,   117,   708,
           234,   108,   155,   125,   171,   133,   112,   129,  3794,   107,
           409,   306,   167,   571,   151,   184,   133,   174,   124,   228,
          3060,   107,  1485,   145,   419,  1430,   111,   114,   396,   678,
           145,   687,   112,   142,   691,  6042,   107,   139,   205,   145,
           368,   140,   225,   561,   115,   109,   439,   108,   111,   113,
           422,   108,   265,   117,   109,   156,   170,  9891,   126,   107,
           184,   163,   133,   174,  1600,   115,   161,   418,   114,   932,
           113,   166,   277,   109,   200,   127,   309,   134,   161,  2760,
           108,   111,   125,   188,   137,   131,   144,   171,   742, 11107,
          1312,   107,   139,  8708, 42161,   151,   452,   642,   112,   114,
           829,   134,   161,   295,   289,   565,   108,   111,   518,   616,
           112,   275, 70933,   122,   372,  1151,   113,  2643,   111,   372,
          2593,   107,   168,   288,  4005,   107,  1032,   126,   131,   116,
           109,  1039,   111,   219,   127,   109,   228,  1352,   125,   131,
           261,  1406,   167,   571,   107,   198,  1435,   140,   230,   314,
           613,   745,   111,   198,   676,   137,   125,   236,   119,   380,
           112,   193,   164,   118,  3134,   496,   600,   921,  6071,   117,
           112,   188,   129,  3200,   130,   783,   130,   125,   137,   107,
           413,   823,   215,   125,   288,   172,   215,   111,   120,   126,
         35769,   213,   173,   125,   311,   113,   149,   109,   176,  2266,
           244,   215,   268,   239,   107,   125,   131,   208,   146,  3403,
           215,   112,   411,   134,   149,   107,   125,   188,   245,   112,
           235,   175,   265,   117,   647,   120,   125,   246,   129,  1727,
           686,   112,   161,   773,   262,   125,   133,   174, 27397,   269,
           141,   161,   282,   813,   314,   107,   125,  2665,   109,  1302,
           125,  2395,   117,   188,   114,   199,   246,   125,   275,   160,
           136,   108,   111,   180,   175,   265,   188,  5489, 40698,   247,
          3143,   152, 59002,   108,   120,   192,   129,  9938,   107,  1063,
           117,   154,  2011,   175,   119,   245,   107, 68319,   112,  1140,
           574,   107,     1]], device='cuda:0')
	response_tensors: 
	 tensor([[    0,   125,   172,   114,  2092,   108,   155,   125,   272,   131,
           144,   235,   175,   265,   117,   647,   125,   246,   129,  1727,
           686,   112,   161,   773,   262,   125,   133,   174, 27397,   269,
           141,   161,   282,   813,   107,   463,   246,   125,   171,   152,
             1]], device='cuda:0')
	query_tensors: 
	 tensor([[  125,   687,   135,   270,   114,  3074,   525,   111,  1371,   465,
           112,   188,  1127,  3977,   161,   813,   299,   135,   109,   278,
           204,   109,   422,   113,   156,  1339,   107,   651,   109,  1197,
           108,   125,   374,   165,   161,  3051,   196,  7353,  7503,   111,
           140,   115,  2241,   108,   167,   125,   905,   247,   135,  2631,
           112,   558,   215,   107,   125,   935,   112,   161,   480,   124,
           109,  1342,   108,   280,   539,   244,   125,   179,   247,   125,
           179,   114,   443,   135,   161, 37074,  9996,  3403,   213,   265,
           196,  2342,   640,   112,  7923,   122,   215,  5040,   107,   139,
           352,   396,   125, 11397,   238,   435,   118,   109,  6891,   108,
           111,   237,   109,   352,   242,   642,   247,   112,  2631,   107,
         20542,  6003,   114, 17437,   113, 12187, 12020, 16320,   114,   242,
           312,  2371,   164,  7315,  2631,   111,  7325,   165,   107,   434,
           125,   687,   247,   238,   112,   179,   114,   494,   125,  2371,
           164,   622,   115,   114,  3564,   241,   161,  9996,   117,   124,
         59884,   262,   113,   161,  3051,   108,   161,  3051,   140,   188,
          3699,   252,   164,   115,   215,   418,   108,   111,   161,  4457,
           140,   848,   249,   188,  7432,   134,   579,   107,   125,  1127,
           518,   161,   525,   456,  1290,   156,   132,   228,   200,   108,
           111,  4961, 77066,   833,   107,  3440,   384,   590,   113,   188,
          6356,  6003,   111,  9744,   160,   109,   480,   269,   125,  1477,
           419,   114,  3951,   164,   109, 62633,   135,   161, 37074,  9996,
           107,   452,   140,   114,  2926,  2581,   115,   161,   271,   111,
           265,  1543,   213,   236,   120,   125,   140,   115,   114,   295,
           241,   125,   196,   112,   201,   514,   112,   927,   161,   348,
           113,   271,   111,   120, 12174,  2177,   131,   144,   245,   213,
           112,   129,   136,   230,   107,   412,   125,   419,   124,   109,
           494,  6662,   108,   374,   114,  3074,  4644,   494,   111,   547,
           375,   357,   166,   107,  3440,   530,   590,   124,   125,   140,
           350,   112,   388, 10292,   114,   295,   122,   156,   113,   109,
           209,   594,   532,   131,   252,  1867,   115,  1266,   122,   333,
           161,   308,  1348,   107,  1032,   109,   799,   242,   108,   532,
           131,   261,   196,   280,  7605,   134,   201,   108,   114,   177,
         10099,   108,   161,  3051,   117,   115,   114,   249,   154,  1259,
           295,   111,   125,   131,   261,   146, 13424,   164,   115,   371,
           590,   107, 11064,   113,   109,   841,   117,   108,   145,   133,
           112,  6021,   115,   109,   749,   113, 26785,   107,   226,   133,
           112,   286,   134,   109,   561,   119,   131,   216,  8626,   115,
           271,   111,   508,   112,   749,   189,   111,   290,  1459,   108,
           129,   126,   525,  3905,   108,  4530,   132,   114,  1312, 22374,
           107,   614,   474,   125,   192,   416,   117,   258,   364,   119,
           131,   216,  4177,   160,   111,   188,   275,   122,   126,   107,
             1]], device='cuda:0')
	response_tensors: 
	 tensor([[    0,   125,   687,   135,   270,   114,  3074,   525,   111,  1371,
           465,   112,  1127,  3977,   161,   813,   299,   135,   109,   278,
           204,   109,   422,   113,   156,  1339,   107,   125,   687,   135,
           270,   114,  3074,   525,   111,  1371,   465,   112,  1127,  3977,
           161,   813,   299,   135,   109,   278,   204,   109,   422,   113,
           156,  1339,   107,  1032,   109,   799,   242,   108,   532,   131,
           261,   196,   280,  7605,   134,   201,   108,   114,   177, 10099,
           108,   161,  3051,   117,   115,   114,   249,   154,  1259,   295,
           111,   125,   131,   261,   146, 13424,   164,   115,   371,   590,
           107,     1]], device='cuda:0')
	query_tensors: 
	 tensor([[  125,   131,   261,   174,   429,   135,   109,   278,   118,   114,
           932,   899,   640,   112,   114,   561,  2015,   107,   125,   131,
           208, 11648,   136,   224, 18531,  6272,  2522,   155,   125,   217,
           161, 28279,   328,   107,   125,   687,   190,   201,   112,   670,
           122,   161,  5388,  1409,   109,  2015,   206,   125,   131,   208,
           114, 77866,   107,   452,  2171,   299,   141,  3403,   213,   156,
           113,   150,  1209,   527,   196,   174,  1194,   141,   114,  1976,
           111,  3040,  3134,  1709,   107,   452,   131,   116,  3403,   213,
           111,   383,   134,   213,   172,   265,   131,   116, 10441,   125,
           131,   208,   146,   188, 10643, 63969,   111,   125,   595,   131,
           144,   630,   447,   107,   125,   666,   125,  1606,   170,   265,
          2443,   108,   111,   125,  1264,  1373,  9938,   108,   155,   265,
           309,  2669,   172,   265,   140,  6225,   154,   107,   125,  1606,
           109,   639,   141,   109,  2256,   265,  2540,   108,   155,   109,
           442, 12564,   299,   107,   125,   687,   122,   126,   107,   125,
          1786,   131,   144,  4337,   112,   132,   684,   189,   113,   161,
           594,   115,   114,   324,   390,   108,   471,   148,   174,   514,
           107,   125,   275,   338,   111,   473,   109,  2648,   112,   236,
           175,   186,   117,   114,  1082,  1409,   109,  1590,  1065,   112,
          6212,   126,   131,   116,   170,   125,   666,   126,   140,   107,
           125,   235,   265,   148,   114,   360,  2045,   167,   125,   666,
          1556,   145,   192,   171,   364,   948,   118,   215,   107,   139,
           442,   120,   125,  1148,   124,   161,  1236,  1457, 35828,  6920,
           112,   161,  1525,   130,   126,   140,   146,   170,   125,   140,
          1234,   134,   149,   107,   139,   442,   111,   109,  2256,   634,
           112,   213,   195,   302,  1401,   107,   168,   140,   589,   156,
           113,   161,   234,   594,   111,  2816,   108,   114,  1209,   639,
           107,   485,   131,   116,   447,   161,  5388,  1214,   154,   135,
           213,   107,  2786,  2168,   231,   459,   107,   125,   393, 27383,
           204,   126,   115,   956,   155,   109,   617,   120,   125,   374,
           165,   135, 26282,   152,   110, 73535,   107,   125,   235,   115,
           161,   773,   265,  1417,   131,   144,   188,  1123,   124,  3815,
           107,  8724,   196,   112,   133,  1871,  1401,   155,   120,   131,
           116,   114,   664,   176,   584,   107,     1]], device='cuda:0')
	response_tensors: 
	 tensor([[    0,   600,  5388,   898,   213,   156,   113,   161,  1209,   527,
           196,   174,  1194,   141,   114,  1976,   111,  3040,   107,   125,
           666,   125,  1606,   170,   265,  2443,   108,   155,   109,   442,
         12564,   299,   111,   125,   374,   165,   126,  1417,   131,   144,
           170,   125,   666,   126,   140,   107,     1]], device='cuda:0')
	query_tensors: 
	 tensor([[ 8087,   107,   125,  1094,   136,   303,   114, 81075,   728,   122,
           109,   217,   112,   179,   181,  1851,   113,  2708,   124,   180,
           117,  3114,   115,   161,   271,   108,   111,   112,   179,   181,
          4234, 28017,   124,   180,   117,   114,   221,  1987,   306,   313,
           124,   317,   161,  1750,   111,   125,   107,  1027,  1688,   151,
           184,   127,   302,  1466,   107, 33154,   118,   377,   231,   107,
           384,   811,  5881,  3185,   124,   215,   477, 23795,   111,  1195,
           158,   135,   114,  1331,  1289,   233,   280,  2220,   317,   214,
         12254,   111,   950,   158,   250,   600,  1750,   111,   125,   133,
           329,   174,   291,   107, 65214,   116,  4350,   111,   149,   120,
           107,   125,   346,  4497,   247,   112,   114,  5709,   107,   452,
           117,  7432,   111,  7695,   112,   114,  5709,   107,   184,   131,
           261,   329, 82482,  6272,   279,   109, 13205,   317,   214,   107,
           600, 12720,   112,  8602,   148,   329,   174,   142,   797,   122,
           215,   130,   249,   130,   215, 12720,   112,  3692,   148,   174,
           122,   213,   107,   184,   131,   261,   174,   224,   377,   231,
           111,  2244,   384,  1995,   404,   108,   802,   125,   393,   125,
           133,  2455,   114, 24668,   491,   115,   109,   289,  1204,   590,
           120,   125,   967,   696,  1454,   107,  3440,  1204,   590,   754,
           265,   547,   313,   112,   114, 26136,  1588,   111,   198, 67297,
           112,  2433,   194,   110,   107,   125,   346,   142, 35615,   108,
          2409,   313,   112,   114, 51417,   399,   107,  1032,   112,   129,
          2335,   108,   870,  2971,   108,   136,  1545,   215,   118,   109,
           340,  4471,   206,   265,   140,   478,  7695,   111,  7432,   111,
           125,   666,   143,  1897,   213,   158,   120,   126,   140,   114,
           255,   411,   254,   175,   125,   595,   131,   144,   857,   215,
          6621,   107,   398,   166,   148,  1871,   124,   265,   148,   460,
           154,   111,   154, 59588,   120,   125,   129,  1065,   115,   109,
          1588,   107,  1032,   272,   131,   144,   179,   213,  1401,   108,
           157,   127,   948,   200,   108,   155,   125,   272,   131,   144,
           537,   153,  6621,   233,   125,   393,   172,   142, 30958,   108,
           172,   114,  1644,   465,   313,   112,   114,  7555, 80592,  8157,
           107,  1308,   150,  5233,   134,   238,   221,   890, 24604,   190,
          5233,   113,   682,   186,   117,   114,  8214,   132,   146,   107,
           182,   112,   213,   117,   146,   142,   428,   906,   108,  3001,
           117,   126,   112,   215,   143, 10960,   122,   291,  2743,   250,
           125,   235,   120,   265,  1728,   213,   112,   411,   108,   192,
           298,   213,   112,   258,  2433,   122,   215,   111,  1305,   215,
           774,  1604,   113, 26136,  6242,   120,   215,  1588,   117, 15001,
           122,   108,   155,   125,   576,   131,   144,   171,   120,   188,
           112,   528,   215,   107,   125,   275,   112,   109,  7635,  1588,
           455,   262,   125,   235,   120,   126,   117,   356,   112,   215,
           108,   155,   125,   131,   208,   146,   313,   112, 12743,   160,
           161,  6621,   107,   651,   349,   113,   136,   145,   133,   114,
           344,   113,   176,   618,   143,  3663,  2875,   161,   328,   158,
           733,   111,   125,   346,  1234,   125,   133,   209,   419,  1250,
           166,   112,   632,   112,   179,   165,   113,   136,   108,   162,
           117,   115,   149, 11206,   395,   213,   221, 13197,   107,   184,
           133,   331,   221,   221,   686,   112, 17826,   204,   109,   289,
           324,   590,   108,   134,   156,  1348,   125,   140,   383,   118,
          5120,   108,   111,   297,   113,   213,  7234,   120,   265,   192,
           129,  8985,   347,   213,   107, 11857,   125,   346,  4359,   682,
           125,   246,   188,  3540,   108,   132,   682,   966,   165,   186,
           174,   224,   233,   111,  7646,   233,   114,  2104,   121,  4527,
           943,  3503,  3775,   943,  3444, 17158,   120,   137,   225, 13006,
           179,   181,  6849,   124,   136,   107,  3650,   445,   145,   192,
           302,   129,  1612,   505,   143, 10960,   146,  1889,   141,   189,
           495,   158,  2076,   125,   368,   107,     1]], device='cuda:0')
	response_tensors: 
	 tensor([[    0,   600,  1750,   113,   377,   231,   117,  1935,   154,   111,
           154, 59588,   120,   125,   129,  1065,   115,   109,  1588,   107,
           125,   272,   131,   144,   235,   175,   125,   246,   188,  3540,
           108,   132,   175,   265,   192,   129,  8985,   347,   213,   107,
             1]], device='cuda:0')

