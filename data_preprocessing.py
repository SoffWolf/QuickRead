# -*- coding: utf-8 -*-
"""data_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zcIjhCecAXYQkxcACXGwR2hfkfsbI5fm
"""

## pip install datasets transformers[sentencepiece]
from datasets import load_dataset, DatasetDict,load_from_disk
#from zipfile import ZipFile
import os

def custom_load_dataset():
    reddit_dataset_raw = load_dataset("reddit"), #download_mode="reuse_cache_if_exists")
    print(reddit_dataset_raw)
    return reddit_dataset_raw

def remove_columns(dataset, column_names):
    '''
    dataset = raw dataset input
    column_names = list of strings, e.g. ["author", "body","subreddit_id","id", "normalizedBody"]
    '''
    reddit_dataset = dataset.remove_columns(column_names)
    print(reddit_dataset)
    return reddit_dataset

# Filter out all rows where either summary or content is blank
def filter_blanks(example):
    return example['content'] is not None and example['summary'] is not None

# Add columns for whitelist
def add_whitelisted(example):
    return {"whitelisted": example['subreddit'] in white_list}

# Add columns for "summary_len" and "content_len" columns
def compute_len(example):
    return {"summary_len": len(example["summary"].split()),
          "content_len": len(example["content"].split())}

# Split dataset into train, test, validation sets: 
def train_test_val_split(dataset, train_size, test_size, val_size):
    assert sum(train_size, test_size, val_size) > 1, "Sum of train_size, test_size, val_size must be 1"
    train_test_valid = dataset.train_test_split(shuffle = True, seed = 50, test_size=test_size+val_size)
    print(train_test_valid)

    # Split the 30% test + valid in half test, half valid
    test_valid = train_test_valid['test'].train_test_split(shuffle = True, seed = 50, test_size=(test_size/(test_size+val_size))*100)
    print(test_valid)

    # gather everyone if you want to have a single DatasetDict
    splitted_dataset = DatasetDict({
        'train': train_test_valid['train'],
        'test': test_valid['test'],
        'valid': test_valid['train']})
    return splitted_dataset

if __name__=='__main__':
    white_list = ["relationships","AskReddit","relationship_advice","tifu","dating_advice","personalfinance","Advice","legaladvice","offmychest","loseit","jobs","self","BreakUps","askwomenadvice","dogs","running","pettyrevenge","needadvice","travel","Parenting","weddingplanning","Pets","Dogtraining","cats","AskDocs","college","GetMotivated","books","Cooking"]

    # Load dataset raw
    reddit_dataset_raw = custom_load_dataset()

    # Remove extra columns
    reddit_dataset = remove_columns(reddit_dataset_raw['train'], ["author", "body","subreddit_id","id", "normalizedBody"])

    ##### NEED TO BE REMOVED 
    reddit_dataset = reddit_dataset.shuffle().select(range(100)) 
    print("Original len = ", len(reddit_dataset))
    ##############################

    # Clean blanks
    reddit_clean_blanks = reddit_dataset.filter(lambda x: filter_blanks(x))
    print("After filtering blanks len = ", len(reddit_clean_blanks))
    print(reddit_clean_blanks[0])

    # Add whitelisted column
    reddit_add_whitelisted = reddit_clean_blanks.map(add_whitelisted)
    print(reddit_add_whitelisted[0])

    # Filter rows that are in whitelist
    reddit_whitelisted = reddit_add_whitelisted.filter(lambda x: x["whitelisted"] is True)
    print("After keeping only whitelisted rows = ", len(reddit_whitelisted))
    print(reddit_whitelisted[0])

    # Add length columns for content and summary
    sample_add_len = reddit_whitelisted.map(compute_len)
    print(sample_add_len.num_rows)

    # Filter out rows with content length more than 512 tokens 
    sample_right_len = sample_add_len.filter(lambda x: x["content_len"] < 512)
    print(sample_right_len.num_rows)

    # Filter summaries that are less than 24 tokens or more than 48 tokens
    sample_right_len = sample_right_len.filter(lambda x: x["summary_len"] < 48 and x["summary_len"] > 24)
    print(sample_right_len.num_rows)
    
    print(sample_right_len[0])

    # Remove duplicates: --> SKIP for now

    # Remove columns
    dataset = remove_columns(sample_right_len, ["whitelisted", "summary_len","content_len", 'subreddit'])
    print(dataset.num_rows)

    # Split dataset into train, test, val sets: 
    dataset = train_test_val_split(dataset, train_size=0.7, test_size=0.15, val_size=0.15)
    print(dataset)

    # Save to disk
    dataset.save_to_disk("reddit_clean")

    # Zip datatset
    print(f"Before zipping: {os.listdir()}")
